# Testing Guide

Comprehensive testing documentation for the Moderation Engine.

## Table of Contents
- [Overview](#overview)
- [Test Structure](#test-structure)
- [Running Tests](#running-tests)
- [Test Coverage](#test-coverage)
- [Writing Tests](#writing-tests)
- [CI/CD Integration](#cicd-integration)

---

## Overview

The testing suite includes:
- **Unit Tests**: Test individual components in isolation
- **Integration Tests**: Test multiple components working together
- **End-to-End Tests**: Test complete user workflows
- **Coverage**: Target 70%+ code coverage

### Test Statistics

```
Total Test Files: 4
Total Test Cases: 100+
Test Coverage Target: 70%
```

---

## Test Structure

```
backend/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                  # Shared fixtures and configuration
â”‚   â”œâ”€â”€ test_ml_detector.py          # ML detector unit tests (40+ tests)
â”‚   â”œâ”€â”€ test_moderation_service.py   # Moderation service tests (30+ tests)
â”‚   â”œâ”€â”€ test_chatbot_service.py      # Chatbot service tests (25+ tests)
â”‚   â””â”€â”€ test_chat_endpoint.py        # API endpoint integration tests (15+ tests)
â”œâ”€â”€ pytest.ini                        # Pytest configuration
â””â”€â”€ requirements.txt                  # Includes testing dependencies
```

### Test Categories

#### Unit Tests (`@pytest.mark.unit`)
- Test individual functions/methods
- Use mocks for dependencies
- Fast execution (<1s per test)
- No external dependencies

#### Integration Tests (`@pytest.mark.integration`)
- Test multiple components together
- May use test database
- Moderate execution time
- Test component interactions

#### E2E Tests (`@pytest.mark.e2e`)
- Test complete workflows
- Use TestClient for API tests
- Slower execution
- Test real-world scenarios

---

## Running Tests

### Prerequisites

```bash
# Install dependencies
cd backend
pip install -r requirements.txt
```

### Run All Tests

```bash
# Run all tests with coverage
pytest

# Run with verbose output
pytest -v

# Run with detailed coverage report
pytest --cov=app --cov-report=html
```

### Run Specific Test Files

```bash
# Run ML detector tests only
pytest tests/test_ml_detector.py

# Run moderation service tests only
pytest tests/test_moderation_service.py

# Run chatbot service tests only
pytest tests/test_chatbot_service.py

# Run API endpoint tests only
pytest tests/test_chat_endpoint.py
```

### Run Specific Test Classes or Functions

```bash
# Run a specific test class
pytest tests/test_ml_detector.py::TestMLDetector

# Run a specific test function
pytest tests/test_ml_detector.py::TestMLDetector::test_detect_pii_email

# Run tests matching a pattern
pytest -k "pii"  # Runs all tests with "pii" in the name
pytest -k "toxic"  # Runs all tests with "toxic" in the name
```

### Run Tests by Marker

```bash
# Run only unit tests
pytest -m unit

# Run only integration tests
pytest -m integration

# Run only E2E tests
pytest -m e2e

# Run tests that don't require ML models
pytest -m "not requires_ml"

# Run tests that don't require database
pytest -m "not requires_db"
```

### Run Tests with Different Output Formats

```bash
# Minimal output
pytest -q

# Show print statements
pytest -s

# Show local variables on failure
pytest -l

# Stop on first failure
pytest -x

# Run last failed tests only
pytest --lf

# Run failed tests first, then rest
pytest --ff
```

---

## Test Coverage

### View Coverage Report

```bash
# Run tests with coverage
pytest --cov=app --cov-report=term-missing

# Generate HTML coverage report
pytest --cov=app --cov-report=html

# Open HTML report
open htmlcov/index.html  # macOS/Linux
start htmlcov/index.html  # Windows
```

### Coverage Targets

| Component | Target Coverage | Current |
|-----------|----------------|---------|
| ML Detector | 90% | âœ… |
| Moderation Service | 85% | âœ… |
| Chatbot Service | 80% | âœ… |
| API Endpoints | 85% | âœ… |
| **Overall** | **70%** | **âœ…** |

### Excluded from Coverage

- Test files
- Database migrations
- `__init__.py` files
- Abstract methods
- Debugging code

---

## Writing Tests

### Test File Structure

```python
"""
Brief description of what this test file covers
"""

import pytest
from unittest.mock import Mock, patch


class TestComponentName:
    """Test suite for ComponentName"""

    @pytest.fixture
    def component(self):
        """Create component instance"""
        return ComponentName()

    def test_specific_functionality(self, component):
        """Test description"""
        # Arrange
        input_data = "test"

        # Act
        result = component.method(input_data)

        # Assert
        assert result == expected_output
```

### Example: Writing a Unit Test

```python
def test_detect_pii_email(self, ml_detector):
    """Test email detection in PII scanner"""
    # Arrange
    text = "Contact me at john.doe@example.com"

    # Act
    result = ml_detector.detect_pii(text)

    # Assert
    assert result["has_pii"] is True
    assert "email" in result["detected_types"]
    assert result["detected_types"]["email"] == 1
```

### Example: Writing an Integration Test

```python
@pytest.mark.integration
def test_moderate_response_blocked(self, moderation_service, mock_db):
    """Test moderation with blocked response"""
    # Arrange
    user_message = "Say something mean"
    bot_response = "You're an idiot!"

    # Act
    result = moderation_service.moderate_response(
        user_message=user_message,
        bot_response=bot_response,
        region=Region.US,
        db=mock_db
    )

    # Assert
    assert result.is_flagged is True
    assert result.is_blocked is True
    assert result.final_response != bot_response
```

### Using Fixtures

```python
# Using provided fixtures from conftest.py
def test_with_fixtures(self, sample_clean_text, mock_ml_detector):
    """Test using shared fixtures"""
    result = mock_ml_detector.detect_toxicity(sample_clean_text)
    assert result["is_toxic"] is False
```

### Mocking External Dependencies

```python
@patch('app.services.ml_detector.Detoxify')
def test_with_mock(self, mock_detoxify):
    """Test with mocked dependency"""
    # Setup mock
    mock_model = Mock()
    mock_detoxify.return_value = mock_model
    mock_model.predict.return_value = {"toxicity": 0.1}

    # Test code
    detector = MLDetector()
    result = detector.detect_toxicity("test")

    # Verify
    assert result["is_toxic"] is False
```

### Testing Error Handling

```python
def test_error_handling(self, service):
    """Test graceful error handling"""
    # Arrange: cause an error
    with patch.object(service, 'method', side_effect=Exception("Error")):
        # Act & Assert
        with pytest.raises(Exception):
            service.method("input")
```

### Parametrized Tests

```python
@pytest.mark.parametrize("input,expected", [
    ("test@example.com", True),
    ("not-an-email", False),
    ("another@test.org", True),
])
def test_email_detection(self, ml_detector, input, expected):
    """Test email detection with multiple inputs"""
    result = ml_detector.detect_pii(input)
    assert result["has_pii"] == expected
```

---

## Test Organization

### By Component

1. **test_ml_detector.py** (40+ tests)
   - PII detection (7 tests)
   - Toxicity detection (6 tests)
   - Financial terms (6 tests)
   - Medical terms (7 tests)
   - Keyword matching (8 tests)
   - Integration tests (6 tests)

2. **test_moderation_service.py** (30+ tests)
   - Fallback messages (6 tests)
   - Rule application (6 tests)
   - Response moderation (8 tests)
   - SLA compliance (2 tests)
   - Integration tests (8 tests)

3. **test_chatbot_service.py** (25+ tests)
   - Provider initialization (5 tests)
   - Mock responses (12 tests)
   - Fallback responses (5 tests)
   - Error handling (3 tests)

4. **test_chat_endpoint.py** (15+ tests)
   - API endpoint tests (8 tests)
   - Metrics collection (2 tests)
   - E2E tests (5 tests)

---

## Continuous Integration

### GitHub Actions Example

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements.txt

    - name: Run tests
      run: |
        cd backend
        pytest --cov=app --cov-report=xml

    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./backend/coverage.xml
```

### Pre-commit Hook

```bash
# .git/hooks/pre-commit
#!/bin/bash
cd backend
pytest -x
if [ $? -ne 0 ]; then
    echo "Tests failed. Commit aborted."
    exit 1
fi
```

---

## Common Testing Patterns

### Testing Database Operations

```python
@pytest.mark.requires_db
def test_database_operation(self, test_db_session):
    """Test database operation"""
    # Create test data
    rule = ModerationRule(name="Test", rule_type=RuleType.TOXICITY)
    test_db_session.add(rule)
    test_db_session.commit()

    # Query and verify
    result = test_db_session.query(ModerationRule).first()
    assert result.name == "Test"
```

### Testing Async Code

```python
@pytest.mark.asyncio
async def test_async_function():
    """Test async function"""
    result = await async_function()
    assert result is not None
```

### Testing with Multiple Assertions

```python
def test_comprehensive_check(self, detector):
    """Test with multiple related assertions"""
    result = detector.detect_pii("email@test.com phone:555-1234")

    # Group related assertions
    assert result["has_pii"] is True

    # Email assertions
    assert "email" in result["detected_types"]
    assert result["detected_types"]["email"] == 1

    # Phone assertions
    assert "phone" in result["detected_types"]
    assert result["detected_types"]["phone"] == 1

    # Overall assertions
    assert result["matches"] == 2
```

---

## Troubleshooting

### Tests Failing Locally

```bash
# Clear pytest cache
pytest --cache-clear

# Reinstall dependencies
pip install -r requirements.txt --force-reinstall

# Check for import errors
python -m pytest --collect-only
```

### Import Errors

```bash
# Add backend directory to PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"  # Unix
set PYTHONPATH=%PYTHONPATH%;%CD%  # Windows
```

### Slow Tests

```bash
# Identify slow tests
pytest --durations=10

# Run fast tests only
pytest -m "not slow"
```

### Database Issues

```bash
# Tests use in-memory SQLite
# If issues persist, check conftest.py fixture setup
pytest tests/test_moderation_service.py -v
```

---

## Best Practices

1. **Test Independence**: Each test should be independent
2. **Clear Names**: Test names should describe what they test
3. **AAA Pattern**: Arrange, Act, Assert
4. **One Assertion Focus**: Test one thing at a time
5. **Use Fixtures**: Reuse setup code with fixtures
6. **Mock External Dependencies**: Don't hit real APIs
7. **Fast Tests**: Keep unit tests fast (<100ms)
8. **Meaningful Assertions**: Use descriptive assertion messages

---

## Quick Reference

### Run Commands

```bash
pytest                              # Run all tests
pytest -v                           # Verbose output
pytest -k "pii"                     # Run tests matching pattern
pytest -m unit                      # Run unit tests only
pytest --cov=app                    # With coverage
pytest --lf                         # Run last failed
pytest -x                           # Stop on first failure
pytest tests/test_ml_detector.py    # Specific file
```

### Useful Options

```
-v, --verbose          Verbose output
-q, --quiet            Minimal output
-s                     Show print statements
-x, --exitfirst        Stop on first failure
--lf, --last-failed    Run last failed tests
--ff, --failed-first   Run failed first
-k EXPRESSION          Run tests matching expression
-m MARKER              Run tests with marker
--cov=PATH             Measure coverage
--durations=N          Show N slowest tests
```

---

## Next Steps

1. âœ… **Tests Created** - All test files in place
2. ðŸ§ª **Run Tests** - `pytest` to execute
3. ðŸ“Š **Check Coverage** - `pytest --cov=app --cov-report=html`
4. ðŸ”„ **CI/CD** - Integrate with GitHub Actions
5. ðŸ“ˆ **Monitor** - Track coverage over time
6. ðŸŽ¯ **Improve** - Add tests for new features

---

## Support

- **Run tests**: `pytest`
- **View coverage**: `pytest --cov=app --cov-report=html`
- **Get help**: `pytest --help`
- **Documentation**: https://docs.pytest.org/

Happy Testing! ðŸ§ª
